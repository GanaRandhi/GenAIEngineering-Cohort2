{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It involves developing algorithms and models that can understand, interpret, and generate human language.\n",
    "\n",
    "## What you'll learn in this notebook:\n",
    "1. Text Preprocessing\n",
    "2. Tokenization\n",
    "3. Stop Words Removal\n",
    "4. Stemming and Lemmatization\n",
    "5. Bag of Words (BoW)\n",
    "6. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "7. Basic Sentiment Analysis\n",
    "8. Introduction to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ganar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ganar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ganar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ganar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ganar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Text preprocessing is a crucial step in NLP. It involves cleaning and preparing text data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Natural Language Processing (NLP) is AMAZING! \n",
      "It's a fascinating field that deals with teaching computers to understand human language. \n",
      "Visit https://www.example.com for more info. Contact: john@email.com #NLP #AI\n"
     ]
    }
   ],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"Natural Language Processing (NLP) is AMAZING! \n",
    "It's a fascinating field that deals with teaching computers to understand human language. \n",
    "Visit https://www.example.com for more info. Contact: john@email.com #NLP #AI\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned text:\n",
      "natural language processing nlp is amazing its a fascinating field that deals with teaching computers to understand human language visit for more info contact\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing function\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_text = preprocess_text(sample_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'is',\n",
       " 'amazing',\n",
       " 'its',\n",
       " 'a',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " 'that',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'teaching',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " 'visit',\n",
       " 'for',\n",
       " 'more',\n",
       " 'info',\n",
       " 'contact']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens (words, sentences, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ganar/nltk_data'\n    - 'c:\\\\GenAI\\\\GenAIEngineering-Cohort2\\\\Week1\\\\Week_1_env\\\\nltk_data'\n    - 'c:\\\\GenAI\\\\GenAIEngineering-Cohort2\\\\Week1\\\\Week_1_env\\\\share\\\\nltk_data'\n    - 'c:\\\\GenAI\\\\GenAIEngineering-Cohort2\\\\Week1\\\\Week_1_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ganar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Word tokenization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m words = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWord tokens:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GenAI\\GenAIEngineering-Cohort2\\Week1\\Week_1_env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GenAI\\GenAIEngineering-Cohort2\\Week1\\Week_1_env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GenAI\\GenAIEngineering-Cohort2\\Week1\\Week_1_env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GenAI\\GenAIEngineering-Cohort2\\Week1\\Week_1_env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GenAI\\GenAIEngineering-Cohort2\\Week1\\Week_1_env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GenAI\\GenAIEngineering-Cohort2\\Week1\\Week_1_env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ganar/nltk_data'\n    - 'c:\\\\GenAI\\\\GenAIEngineering-Cohort2\\\\Week1\\\\Week_1_env\\\\nltk_data'\n    - 'c:\\\\GenAI\\\\GenAIEngineering-Cohort2\\\\Week1\\\\Week_1_env\\\\share\\\\nltk_data'\n    - 'c:\\\\GenAI\\\\GenAIEngineering-Cohort2\\\\Week1\\\\Week_1_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ganar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(cleaned_text)\n",
    "print(\"Word tokens:\")\n",
    "print(words)\n",
    "print(f\"\\nNumber of words: {len(words)}\")\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"\\nSentence tokens:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"{i+1}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop Words Removal\n",
    "\n",
    "Stop words are common words that don't carry much meaning (e.g., 'the', 'is', 'at', 'which')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Number of stop words: {len(stop_words)}\")\n",
    "print(f\"Sample stop words: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(f\"\\nOriginal words ({len(words)}): {words}\")\n",
    "print(f\"\\nFiltered words ({len(filtered_words)}): {filtered_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming and Lemmatization\n",
    "\n",
    "Both techniques reduce words to their base form, but lemmatization is more sophisticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "example_words = ['running', 'runs', 'ran', 'easily', 'fairly', 'better', 'worse']\n",
    "\n",
    "print(\"Word\\t\\tStem\\t\\tLemma\")\n",
    "print(\"-\" * 40)\n",
    "for word in example_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word}\\t\\t{stem}\\t\\t{lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to our filtered words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Lemmatized words:\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bag of Words (BoW)\n",
    "\n",
    "Bag of Words is a simple way to represent text data as numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"Machine learning and NLP go hand in hand.\",\n",
    "    \"Text analysis is a key part of NLP.\",\n",
    "    \"Deep learning revolutionized natural language processing.\"\n",
    "]\n",
    "\n",
    "# Create BoW representation\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
    "print(\"Bag of Words representation:\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF gives more weight to words that are important to a document but not common across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ").round(3)\n",
    "\n",
    "print(\"TF-IDF representation:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize important words in each document\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (doc, ax) in enumerate(zip(documents, axes)):\n",
    "    # Get TF-IDF scores for this document\n",
    "    scores = tfidf_df.iloc[idx]\n",
    "    top_words = scores.nlargest(5)\n",
    "    \n",
    "    # Plot\n",
    "    top_words.plot(kind='barh', ax=ax)\n",
    "    ax.set_title(f'Document {idx+1}: Top TF-IDF words')\n",
    "    ax.set_xlabel('TF-IDF Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Basic Sentiment Analysis\n",
    "\n",
    "Let's perform simple sentiment analysis using NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample texts for sentiment analysis\n",
    "texts = [\n",
    "    \"I love this product! It's absolutely amazing.\",\n",
    "    \"This is terrible. I hate it.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"The service was good but the food was bad.\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "# Analyze sentiment\n",
    "results = []\n",
    "for text in texts:\n",
    "    scores = sia.polarity_scores(text)\n",
    "    scores['text'] = text\n",
    "    results.append(scores)\n",
    "\n",
    "# Create DataFrame\n",
    "sentiment_df = pd.DataFrame(results)\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(sentiment_df[['text', 'neg', 'neu', 'pos', 'compound']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar plot of compound scores\n",
    "sentiment_df['compound'].plot(kind='bar', ax=ax1, color=['red' if x < 0 else 'green' for x in sentiment_df['compound']])\n",
    "ax1.set_title('Compound Sentiment Scores')\n",
    "ax1.set_xlabel('Text Index')\n",
    "ax1.set_ylabel('Compound Score')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Stacked bar plot of sentiment components\n",
    "sentiment_df[['neg', 'neu', 'pos']].plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('Sentiment Components')\n",
    "ax2.set_xlabel('Text Index')\n",
    "ax2.set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Word Frequency Analysis\n",
    "\n",
    "Let's analyze word frequencies in a larger text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample paragraph for analysis\n",
    "paragraph = \"\"\"\n",
    "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language. NLP combines computational \n",
    "linguistics with statistical, machine learning, and deep learning models. The goal is to enable \n",
    "computers to process and analyze large amounts of natural language data. Common NLP tasks include \n",
    "tokenization, parsing, lemmatization, speech recognition, and machine translation. Modern NLP heavily \n",
    "relies on machine learning algorithms and neural networks, particularly transformer models like BERT \n",
    "and GPT. These models have revolutionized how computers understand and generate human language.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess and tokenize\n",
    "cleaned_paragraph = preprocess_text(paragraph)\n",
    "words = word_tokenize(cleaned_paragraph)\n",
    "\n",
    "# Remove stop words\n",
    "words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words)\n",
    "top_words = word_freq.most_common(10)\n",
    "\n",
    "# Create visualization\n",
    "words, counts = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.title('Top 10 Most Frequent Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Introduction to Word Embeddings\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example of one-hot encoding vs embeddings concept\n",
    "vocabulary = ['king', 'queen', 'man', 'woman', 'royal']\n",
    "\n",
    "# One-hot encoding\n",
    "print(\"One-hot encoding:\")\n",
    "for i, word in enumerate(vocabulary):\n",
    "    one_hot = [0] * len(vocabulary)\n",
    "    one_hot[i] = 1\n",
    "    print(f\"{word}: {one_hot}\")\n",
    "\n",
    "print(\"\\nProblems with one-hot encoding:\")\n",
    "print(\"- High dimensionality\")\n",
    "print(\"- No semantic relationships\")\n",
    "print(\"- Sparse representation\")\n",
    "\n",
    "print(\"\\nWord embeddings solve these problems by:\")\n",
    "print(\"- Dense, low-dimensional vectors\")\n",
    "print(\"- Capturing semantic relationships\")\n",
    "print(\"- Similar words have similar vectors\")\n",
    "print(\"\\nPopular word embedding models:\")\n",
    "print(\"- Word2Vec\")\n",
    "print(\"- GloVe\")\n",
    "print(\"- FastText\")\n",
    "print(\"- Transformer-based embeddings (BERT, GPT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we covered the fundamental concepts of NLP:\n",
    "\n",
    "1. **Text Preprocessing**: Cleaning and preparing text data\n",
    "2. **Tokenization**: Breaking text into words and sentences\n",
    "3. **Stop Words Removal**: Filtering out common words\n",
    "4. **Stemming/Lemmatization**: Reducing words to their base form\n",
    "5. **Bag of Words**: Simple numerical representation\n",
    "6. **TF-IDF**: Weighted word importance\n",
    "7. **Sentiment Analysis**: Understanding text emotion\n",
    "8. **Word Embeddings**: Dense vector representations\n",
    "\n",
    "### Next Steps:\n",
    "- Explore advanced models like Word2Vec and BERT\n",
    "- Try named entity recognition (NER)\n",
    "- Build a text classification model\n",
    "- Experiment with text generation\n",
    "- Learn about transformer architectures\n",
    "\n",
    "### Recommended Resources:\n",
    "- NLTK Book: https://www.nltk.org/book/\n",
    "- spaCy Documentation: https://spacy.io/\n",
    "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
    "- Stanford NLP Course: https://web.stanford.edu/class/cs224n/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice Exercise: Analyze your own text\n",
    "# TODO: Replace this with your own text\n",
    "your_text = \"Enter your text here to analyze...\"\n",
    "\n",
    "# Apply what you learned\n",
    "# 1. Preprocess the text\n",
    "# 2. Tokenize it\n",
    "# 3. Remove stop words\n",
    "# 4. Perform sentiment analysis\n",
    "# 5. Create a word frequency visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week_1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
